{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env reward scaling - pettingzoo w/ MADDPG\n",
    "\n",
    "Ensures standard MADDPG works w/ my fold-change reward scaling scheme\n",
    "\n",
    "Scales using MovingFoldChangeReward. \n",
    "\n",
    "### Background\n",
    "In Dualer models I put reward and intrinsic rewards in competition. This means the two values must be matched up. Or put another way, that need to have common units. There are many ways to do that. Let's consider a biological motivated approach to reward normalization inspired by:\n",
    "\n",
    "    - Adler, M., and Alon, U. (2018). Fold-change detection in biological\n",
    "    systems. Current Opinion in Systems Biology 8, 81â€“89.\n",
    "    - Karin, O., and Alon, U. (2021). The dopamine circuit as a reward-taxis navigation system. BioRxiv 439955, 30.\n",
    "\n",
    "We'll try several `envs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Box\n",
    "import numpy as np\n",
    "import torch \n",
    "from tqdm import tqdm\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "\n",
    "import pettingzoo\n",
    "from pettingzoo import mpe\n",
    "import supersuit as ss\n",
    "from supersuit import clip_actions_v0\n",
    "\n",
    "from infoduel_maddpg.utils.buffer import ReplayBuffer\n",
    "from infoduel_maddpg.core import MADDPG\n",
    "\n",
    "from infoduel_maddpg.utils.academic_wrappers import StatePredictionWrapper\n",
    "from infoduel_maddpg.utils.normalize_wrappers import ClipRewardWrapper\n",
    "from infoduel_maddpg.utils.normalize_wrappers import MovingFoldChangeRewardWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"simple_v2\"\n",
    "# env_name = \"simple_tag_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 50\n",
    "n_episodes = 25000 # 25000\n",
    "episode_length = 25\n",
    "batch_size = 1024\n",
    "steps_per_update = 100\n",
    "init_noise_scale = 0.3\n",
    "final_noise_scale = 0.0\n",
    "n_exploration_eps = n_episodes\n",
    "\n",
    "do_fold = True\n",
    "do_clip = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Env = getattr(mpe, env_name)\n",
    "env = Env.parallel_env(continuous_actions=True)\n",
    "env = clip_actions_v0(env)\n",
    "\n",
    "# Fold-change?\n",
    "if do_fold:\n",
    "    env  = MovingFoldChangeRewardWrapper(\n",
    "        env, \n",
    "        intial_reference_reward=-2, \n",
    "        bias_reward=0, \n",
    "    )\n",
    "# Clip?\n",
    "if do_clip:\n",
    "    env = ClipRewardWrapper(env, min_reward=-100, max_reward=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make access to 'space' info in format that is\n",
    "# expected through the codebase\n",
    "action_space = [env.action_space(a) for a in env.possible_agents]\n",
    "observation_space = [env.observation_space(a) for a in env.possible_agents]\n",
    "\n",
    "maddpg = MADDPG.init_from_env(\n",
    "    env,\n",
    "    agent_alg=\"MADDPG\",\n",
    "    adversary_alg=\"MADDPG\",\n",
    "    tau=0.01,\n",
    "    lr=0.01,\n",
    "    hidden_dim=64,\n",
    ")\n",
    "replay_buffer = ReplayBuffer(\n",
    "    n_episodes,\n",
    "    maddpg.nagents,\n",
    "    [obsp.shape[0] for obsp in observation_space],\n",
    "    [acsp.shape[0] if isinstance(acsp, Box) else acsp.n for acsp in action_space],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"----- Running: {env_name} ------\")\n",
    "print(f\"device: cpu\")\n",
    "# print(f\"log_dir: {log_dir}\")\n",
    "\n",
    "episode_rewards = []\n",
    "total_rewards = []\n",
    "for n in tqdm(range(n_episodes)):\n",
    "    # print(f\"Episodes {n + 1}-{n + 2} of {n_episodes}\")\n",
    "\n",
    "    # --- Rollout prep\n",
    "    obs = env.reset()\n",
    "    maddpg.prep_rollouts(device=\"cpu\")\n",
    "    \n",
    "    # Explore (noise) decreases as we go.... \n",
    "    explr_pct_remaining = (\n",
    "            max(0, n_exploration_eps - n) / n_exploration_eps\n",
    "        )\n",
    "    maddpg.scale_noise(\n",
    "        final_noise_scale\n",
    "        + (init_noise_scale - final_noise_scale) * explr_pct_remaining\n",
    "    )\n",
    "    maddpg.reset_noise()\n",
    "\n",
    "    # --- do Rollout\n",
    "    for t in range(episode_length):\n",
    "\n",
    "        # If there are no agents the env\n",
    "        # should be restarted.\n",
    "        if len(env.agents) == 0:\n",
    "            obs = env.reset()\n",
    "\n",
    "        # rearrange observations for maddpg\n",
    "        torch_obs = [\n",
    "            torch.tensor(obs[a], requires_grad=False).unsqueeze(0)\n",
    "            for a in env.possible_agents\n",
    "        ]\n",
    "        # Agents act 'at once' (as parallelized AEC)\n",
    "        torch_agent_actions = maddpg.step(torch_obs, explore=True)\n",
    "        # rearrange actions for zoo environment\n",
    "        agent_actions = {}\n",
    "        for a, ac in zip(env.possible_agents, torch_agent_actions):\n",
    "            # The env will only accept 'env.possible_agents',\n",
    "            # so we filter out the dead\n",
    "            if a in env.possible_agents:\n",
    "                agent_actions[a] = ac.data.numpy().flatten()\n",
    "        # !\n",
    "        next_obs, rewards, dones, infos = env.step(agent_actions)\n",
    "        episode_rewards.append(rewards)\n",
    "        replay_buffer.push(\n",
    "            [obs[a] for a in env.possible_agents],\n",
    "            [agent_actions[a] for a in env.possible_agents],\n",
    "            [rewards[a] for a in env.possible_agents],\n",
    "            [next_obs[a] for a in env.possible_agents],\n",
    "            [dones[a] for a in env.possible_agents],\n",
    "        )\n",
    "\n",
    "        # setup for next step\n",
    "        obs = next_obs\n",
    "        t += 1\n",
    "\n",
    "        # --- Train (ugly code)?\n",
    "        if (\n",
    "            len(replay_buffer) >= batch_size\n",
    "            and (t % steps_per_update) == 1\n",
    "        ):\n",
    "            maddpg.prep_training(device=\"cpu\")\n",
    "            for a_i, a_n in enumerate(env.possible_agents):\n",
    "                sample = replay_buffer.sample(batch_size, \"cpu\")\n",
    "                maddpg.update(sample, a_i, a_n, logger=None)\n",
    "            maddpg.update_all_targets()\n",
    "            maddpg.prep_rollouts(device=\"cpu\")\n",
    "    \n",
    "    # -- Log (ep)\n",
    "    episode_reward = replay_buffer.get_average_rewards(episode_length)\n",
    "    total_rewards.append(deepcopy(episode_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = \"agent_0\"\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(16, 18))\n",
    "\n",
    "times = np.arange(0, len(total_rewards))\n",
    "ax1.plot(times, np.concatenate(total_rewards), '.-', color=\"black\", label=\"reward\")\n",
    "ax1.set_ylabel(\"Mean reward\")\n",
    "ax1.set_xlabel(\"Episode\")\n",
    "# ax1.set_xlim(9900, 10000)\n",
    "# ax1.set_ylim(-3, 0)\n",
    "\n",
    "times = np.arange(0, np.asarray([r[agent] for r in episode_rewards]).size)\n",
    "ax2.plot(times, np.asarray([r[agent] for r in episode_rewards]), '.', color=\"black\", label=\"reward\")\n",
    "ax2.set_ylabel(\"Reward\")\n",
    "ax2.set_xlabel(\"Step\")\n",
    "ax2.set_xlim(0, 4000)\n",
    "ax2.set_ylim(-5, 2)\n",
    "ax2.set_title(\"Early (0, 4000)\")\n",
    "\n",
    "ax3.plot(times, np.asarray([r[agent] for r in episode_rewards]), '.', color=\"black\", label=\"reward\")\n",
    "ax3.set_ylabel(\"Reward\")\n",
    "ax3.set_xlabel(\"Step\")\n",
    "ax3.set_xlim(times.size - 4000, times.size)\n",
    "ax3.set_ylim(-5, 2)\n",
    "ax3.set_title(f\"Late ({times.size - 4000}, {times.size})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e89847e936c8c95f8cbdafeaa6e7c814f27278d8de21943cca2b199a4582e4c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
